{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0f2c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install/Import required packages (uncomment install lines if needed in Colab)\n",
    "import os\n",
    "import math\n",
    "import shutil\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from glob import glob\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print('TensorFlow version:', tf.__version__)\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# If running in Colab and you need to install a different TF, uncomment below (use with caution):\n",
    "# !pip install -q 'tensorflow==2.14.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5282f73a",
   "metadata": {},
   "source": [
    "## 1) Prepare dataset: upload, mount or point to local directory\n",
    "\n",
    "This cell provides two options:\n",
    "- Option A (recommended in Colab): Mount Google Drive and set `DATA_ROOT` to the folder containing `Train data` and `Test_Data`.\n",
    "- Option B: Upload a ZIP file directly to the Colab session and it will be extracted to `/content/dataset`.\n",
    "\n",
    "If you're running locally (not Colab), set `DATA_ROOT` to your dataset root path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305b491c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust these paths according to your environment.\n",
    "# If using Colab and Google Drive:\n",
    "USE_GOOGLE_DRIVE = False  # set to True if mounting Drive in Colab\n",
    "DRIVE_DATA_PATH = '/content/drive/MyDrive/AniLink_dataset'  # change to your Drive path\n",
    "\n",
    "# If uploading a ZIP in Colab, set USE_UPLOAD = True to use the upload cell below.\n",
    "USE_UPLOAD = False\n",
    "\n",
    "# Default local path fallback (if running locally and your folders are on your machine):\n",
    "LOCAL_DATA_PATH = r'C:\\Users\\USER\\Desktop\\AniLink_project\\AniLink'  # adjust if running locally\n",
    "\n",
    "# Final DATA_ROOT will be determined below\n",
    "DATA_ROOT = None\n",
    "\n",
    "if USE_GOOGLE_DRIVE:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    DATA_ROOT = DRIVE_DATA_PATH\n",
    "\n",
    "if USE_UPLOAD:\n",
    "    # This will prompt you to upload a zip file in Colab. Uncomment if needed.\n",
    "    from google.colab import files\n",
    "    uploaded = files.upload()\n",
    "    # find first uploaded zip\n",
    "    for name in uploaded.keys():\n",
    "        if name.endswith('.zip'):\n",
    "            zip_path = name\n",
    "            break\n",
    "    else:\n",
    "        raise RuntimeError('Upload a zip archive containing Train data/ and Test_Data/ folders')\n",
    "    import zipfile\n",
    "    extract_dir = '/content/dataset'\n",
    "    os.makedirs(extract_dir, exist_ok=True)\n",
    "    with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "        z.extractall(extract_dir)\n",
    "    DATA_ROOT = extract_dir\n",
    "\n",
    "if DATA_ROOT is None:\n",
    "    # prefer the local workspace path if it exists\n",
    "    if os.path.exists(LOCAL_DATA_PATH):\n",
    "        DATA_ROOT = LOCAL_DATA_PATH\n",
    "    else:\n",
    "        # fallback: expect user to upload or set DATA_ROOT manually\n",
    "        raise FileNotFoundError('Dataset root not found. Set USE_GOOGLE_DRIVE=True (in Colab) or adjust LOCAL_DATA_PATH to point to your dataset root.')\n",
    "\n",
    "print('DATA_ROOT set to:', DATA_ROOT)\n",
    "\n",
    "# Expecting these folders inside DATA_ROOT: 'Train data' and 'Test_Data' (case-sensitive)\n",
    "TRAIN_DIR = os.path.join(DATA_ROOT, 'Train data')\n",
    "TEST_DIR = os.path.join(DATA_ROOT, 'Test_Data')\n",
    "assert os.path.isdir(TRAIN_DIR), f\"Train directory not found: {TRAIN_DIR}\"\n",
    "assert os.path.isdir(TEST_DIR), f\"Test directory not found: {TEST_DIR}\"\n",
    "print('Train dir classes:', os.listdir(TRAIN_DIR))\n",
    "print('Test dir classes:', os.listdir(TEST_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc43f8d0",
   "metadata": {},
   "source": [
    "## 2) Create ImageDataGenerators with heavy augmentation and automatic 80/20 split\n",
    "\n",
    "We use `ImageDataGenerator` which supports `validation_split`. Heavy augmentation includes rotation, flips, shear, zoom, shifts, and brightness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cea6005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "SEED = 123\n",
    "\n",
    "# Heavy augmentation for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.25,\n",
    "    zoom_range=0.2,\n",
    "    brightness_range=[0.7,1.3],\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False,\n",
    "    fill_mode='reflect',\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "# For validation we only rescale\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\n",
    "# For test set we only rescale\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Flow from directory with 80/20 split\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    subset='training',\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "val_generator = valid_datagen.flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    subset='validation',\n",
    "    seed=SEED,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    TEST_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "num_train = train_generator.samples\n",
    "num_val = val_generator.samples\n",
    "num_test = test_generator.samples\n",
    "print('Train samples:', num_train, 'Val samples:', num_val, 'Test samples:', num_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a3df30",
   "metadata": {},
   "source": [
    "## 3) Build model: MobileNetV3Small (ImageNet) with transfer learning\n",
    "\n",
    "We freeze the base model, train the head, then optionally unfreeze and fine-tune a few layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72ca60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNetV3Small\n",
    "\n",
    "base_model = MobileNetV3Small(input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3), include_top=False, weights='imagenet')\n",
    "base_model.trainable = False  # freeze for initial training\n",
    "\n",
    "inputs = keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
    "x = inputs\n",
    "x = base_model(x, training=False)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68f6ce0",
   "metadata": {},
   "source": [
    "## 4) Train with callbacks (EarlyStopping, ReduceLROnPlateau) and checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5ab46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "checkpoint_path = 'best_model.h5'\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(checkpoint_path, monitor='val_loss', save_best_only=True, verbose=1),\n",
    "    keras.callbacks.EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True, verbose=1),\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7, verbose=1)\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a00688d",
   "metadata": {},
   "source": [
    "### Optional: Unfreeze and fine-tune\n",
    "Unfreeze top layers of the base model and continue training with a smaller learning rate to potentially boost accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23139f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune: unfreeze some layers\n",
    "base_model.trainable = True\n",
    "\n",
    "# Freeze bottom layers, unfreeze top n\n",
    "fine_tune_at = len(base_model.layers) - 20  # tweak as needed\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "    layer.trainable = (i >= fine_tune_at)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=5e-6),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "FT_EPOCHS = 10\n",
    "callbacks_ft = [\n",
    "    keras.callbacks.ModelCheckpoint(checkpoint_path, monitor='val_loss', save_best_only=True, verbose=1),\n",
    "    keras.callbacks.EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True, verbose=1),\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-8, verbose=1)\n",
    "]\n",
    "\n",
    "history_ft = model.fit(\n",
    "    train_generator,\n",
    "    epochs=FT_EPOCHS,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=callbacks_ft\n",
    ")\n",
    "\n",
    "# Merge history objects\n",
    "def merge_histories(h1, h2):\n",
    "    for k in h2.history:\n",
    "        h1.history.setdefault(k, []).extend(h2.history[k])\n",
    "    return h1\n",
    "\n",
    "history = merge_histories(history, history_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfd8531",
   "metadata": {},
   "source": [
    "## 5) Evaluate: accuracy/loss curves, confusion matrix, classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbdd04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model weights\n",
    "model.load_weights(checkpoint_path)\n",
    "\n",
    "# Plot training curves\n",
    "acc = history.history.get('accuracy', [])\n",
    "val_acc = history.history.get('val_accuracy', [])\n",
    "loss = history.history.get('loss', [])\n",
    "val_loss = history.history.get('val_loss', [])\n",
    "epochs_range = range(1, len(acc) + 1)\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs_range, acc, label='Train Acc')\n",
    "plt.plot(epochs_range, val_acc, label='Val Acc')\n",
    "plt.legend(); plt.title('Accuracy')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epochs_range, loss, label='Train Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Val Loss')\n",
    "plt.legend(); plt.title('Loss')\n",
    "plt.show()\n",
    "\n",
    "# Predict on test set\n",
    "test_steps = math.ceil(num_test / BATCH_SIZE)\n",
    "preds = model.predict(test_generator, steps=test_steps, verbose=1)\n",
    "y_pred = (preds.ravel() >= 0.5).astype(int)\n",
    "y_true = test_generator.classes[:len(y_pred)]\n",
    "\n",
    "# Confusion matrix and classification report\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "print('Classification Report:')\n",
    "target_names = ['healthy (0)', 'FMD (1)']\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))\n",
    "\n",
    "# Overall accuracy\n",
    "acc_test = np.mean(y_pred == y_true)\n",
    "print(f'Test accuracy: {acc_test*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5dccc2",
   "metadata": {},
   "source": [
    "## 6) Convert to TensorFlow Lite with quantization (float16) and check size\n",
    "Float16 quantization generally reduces model size significantly and is widely compatible on modern devices. If you need integer quantization for full integer support, a representative dataset generator is required (we include an example commented section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a60552",
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_model_path = 'model_float16.tflite'\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_types = [tf.float16]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open(tflite_model_path, 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "size_mb = os.path.getsize(tflite_model_path) / (1024*1024)\n",
    "print(f'TFLite model saved to {tflite_model_path} ({size_mb:.2f} MB)')\n",
    "\n",
    "if size_mb > 6.0:\n",
    "    print('Warning: model size > 6 MB. Consider using MobileNetV3Small with more aggressive quantization (int8) or pruning.)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564529c0",
   "metadata": {},
   "source": [
    "### (Optional) Full integer quantization with representative dataset\n",
    "Uncomment and use a representative generator if you need int8 quantization. This may produce a smaller model but requires calibration data and may need ops support on target device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b58946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: representative dataset generator (uncomment to use)\n",
    "###\n",
    "### def representative_data_gen():\n",
    "###     for i in range(100):\n",
    "###         img, _ = next(train_generator)\n",
    "###         # img: batch of images\n",
    "###         for j in range(img.shape[0]):\n",
    "###             yield [img[j:j+1].astype(np.float32)]\n",
    "###\n",
    "### converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "### converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "### converter.representative_dataset = representative_data_gen\n",
    "### converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "### converter.inference_input_type = tf.uint8\n",
    "### converter.inference_output_type = tf.uint8\n",
    "### tflite_quant_path = 'model_int8.tflite'\n",
    "### tflite_quant_model = converter.convert()\n",
    "### with open(tflite_quant_path, 'wb') as f:\n",
    "###     f.write(tflite_quant_model)\n",
    "### print('Saved int8 model:', tflite_quant_path, os.path.getsize(tflite_quant_path)/(1024*1024), 'MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c109abb",
   "metadata": {},
   "source": [
    "## 7) TFLite inference on a single image\n",
    "This cell shows how to load the `.tflite` file and run inference on one image. Replace `sample_image_path` with your image path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b96bb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "def load_image_for_model(path, target_size=IMG_SIZE):\n",
    "    img = Image.open(path).convert('RGB')\n",
    "    img = img.resize(target_size)\n",
    "    arr = np.array(img).astype('float32') / 255.0\n",
    "    return arr\n",
    "\n",
    "# Load tflite model\n",
    "interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "def tflite_predict(image_path):\n",
    "    img = load_image_for_model(image_path)\n",
    "    inp = np.expand_dims(img, axis=0).astype(np.float32)\n",
    "    # If the model is float16 quantized, input is float32; conversion is handled by interpreter\n",
    "    interpreter.set_tensor(input_details[0]['index'], inp)\n",
    "    interpreter.invoke()\n",
    "    out = interpreter.get_tensor(output_details[0]['index'])\n",
    "    prob = float(out.ravel()[0])\n",
    "    pred = 1 if prob >= 0.5 else 0\n",
    "    return pred, prob\n",
    "\n",
    "# Example usage (uncomment and set a valid image path):\n",
    "# sample_image_path = '/content/sample.jpg'\n",
    "# pred, prob = tflite_predict(sample_image_path)\n",
    "# print('Predicted class:', pred, 'probability:', prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebfc3fc",
   "metadata": {},
   "source": [
    "## 8) Save Keras model and provide quick local usage notes\n",
    "\n",
    "We also save the Keras model (`best_model.h5`) for inspection and possible re-conversion locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3212923d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Keras model\n",
    "keras_model_path = 'best_model.h5'\n",
    "model.save(keras_model_path)\n",
    "print('Saved Keras model to', keras_model_path, 'size (MB):', os.path.getsize(keras_model_path)/(1024*1024))\n",
    "\n",
    "print('\\nDone. Summary:')\n",
    "print('-', 'Train samples:', num_train)\n",
    "print('-', 'Validation samples:', num_val)\n",
    "print('-', 'Test samples:', num_test)\n",
    "print('-', 'TFLite model:', tflite_model_path, f'({size_mb:.2f} MB)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1eaeebf",
   "metadata": {},
   "source": [
    "**Notes and tips to reach >94% accuracy:**\n",
    "- Ensure quality of dataset (clear lesion images, consistent labeling). Clean mislabeled samples.\n",
    "- If classes are imbalanced, consider `class_weight` in `model.fit` or oversampling augmentations for minority class.\n",
    "- Increase `IMG_SIZE` (e.g., 260 or 288) if lesions are small, but TFLite size may grow.\n",
    "- Consider training longer or unfreezing more layers during fine-tuning.\n",
    "- If TFLite size is still >6MB, try int8 quantization with a representative dataset (see commented code)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
